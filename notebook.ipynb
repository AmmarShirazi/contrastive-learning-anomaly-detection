{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jniaZ8lC-BrH",
        "outputId": "4f16f615-787d-47b7-f45d-d4452cb03016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/'\n",
        "train_df = pd.read_csv(path + 'train.csv')\n",
        "test_df = pd.read_csv(path + 'test.csv')\n",
        "test_labels_df = pd.read_csv(path + 'test_label.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler()\n",
        "features = train_df.columns[1:]  # Excluding timestamp\n",
        "train_df[features] = scaler.fit_transform(train_df[features])\n",
        "test_df[features] = scaler.transform(test_df[features])\n",
        "\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n"
      ],
      "metadata": {
        "id": "ASFqzF2b-IvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def geometric_masking(data, p=0.1):\n",
        "    mask = np.random.geometric(p, size=data.shape) <= 1\n",
        "    return data * mask\n",
        "\n",
        "masked_data = geometric_masking(train_df[features].values)\n"
      ],
      "metadata": {
        "id": "qlXcriyo-MgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(x, x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Dense(inputs.shape[-1])(x)\n",
        "    return x + res\n",
        "\n",
        "inputs = Input(shape=(None, len(features)))\n",
        "x = transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128)\n",
        "outputs = Dense(len(features), activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='mse')\n"
      ],
      "metadata": {
        "id": "9-PoXiug-P04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to appropriate format for training\n",
        "sequence_length = 30\n",
        "X_train = np.array([masked_data[i:i+sequence_length] for i in range(len(masked_data) - sequence_length)])\n",
        "y_train = X_train\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxkACx-o-YRG",
        "outputId": "c17ce60c-bc1e-49fa-a5cb-ee00923f45bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4140/4140 [==============================] - 35s 7ms/step - loss: 0.0023\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aced9263d90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n"
      ],
      "metadata": {
        "id": "mAaCD0MDD4IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def build_generator(latent_dim, output_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, input_dim=latent_dim),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(np.prod(output_shape), activation='tanh'),\n",
        "        Reshape(output_shape)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator(input_shape):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim, (len(features),))\n",
        "discriminator = build_discriminator((len(features),))\n",
        "\n",
        "discriminator.trainable = False\n",
        "gan_input = Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
      ],
      "metadata": {
        "id": "VpOu2qKBEtay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, features, epochs, batch_size, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        idx = np.random.randint(0, features.shape[0], half_batch)\n",
        "        real_data = features[idx]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "        gen_data = generator.predict(noise)\n",
        "\n",
        "        real_y = np.ones((half_batch, 1))\n",
        "        fake_y = np.zeros((half_batch, 1))\n",
        "        d_loss_real = discriminator.train_on_batch(real_data, real_y)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_data, fake_y)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        valid_y = np.ones((batch_size, 1))\n",
        "        g_loss = gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        print(f\"Epoch {epoch} / {epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "features = train_df[train_df.columns[1:]].values\n",
        "\n",
        "train_gan(generator, discriminator, gan, features, epochs=100, batch_size=32, latent_dim=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKKzyJhbEyBj",
        "outputId": "42576009-a16c-49fb-c4af-b300d7b1a819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 0 / 100, D Loss: 0.6078918278217316, G Loss: 0.7971608638763428\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 1 / 100, D Loss: 0.5896534621715546, G Loss: 0.8271079659461975\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 2 / 100, D Loss: 0.5583745241165161, G Loss: 0.8583386540412903\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 3 / 100, D Loss: 0.5177457928657532, G Loss: 0.8893975019454956\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 4 / 100, D Loss: 0.5135910958051682, G Loss: 0.9094705581665039\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 5 / 100, D Loss: 0.48505493998527527, G Loss: 0.9673546552658081\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 6 / 100, D Loss: 0.458295613527298, G Loss: 1.0026311874389648\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 7 / 100, D Loss: 0.42749227583408356, G Loss: 1.1115370988845825\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 8 / 100, D Loss: 0.4055106192827225, G Loss: 1.1801156997680664\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 9 / 100, D Loss: 0.3922591060400009, G Loss: 1.1926604509353638\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Epoch 10 / 100, D Loss: 0.3341425806283951, G Loss: 1.2198644876480103\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 11 / 100, D Loss: 0.3823211193084717, G Loss: 1.3055200576782227\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 12 / 100, D Loss: 0.3231939524412155, G Loss: 1.3611245155334473\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 13 / 100, D Loss: 0.317561537027359, G Loss: 1.4114713668823242\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 14 / 100, D Loss: 0.2630885988473892, G Loss: 1.534620761871338\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Epoch 15 / 100, D Loss: 0.26570603996515274, G Loss: 1.5290956497192383\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 16 / 100, D Loss: 0.2809629663825035, G Loss: 1.6142189502716064\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 17 / 100, D Loss: 0.22974812239408493, G Loss: 1.602752685546875\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Epoch 18 / 100, D Loss: 0.21846972405910492, G Loss: 1.661309003829956\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 19 / 100, D Loss: 0.2071935534477234, G Loss: 1.7363618612289429\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 20 / 100, D Loss: 0.1776537299156189, G Loss: 1.7317986488342285\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch 21 / 100, D Loss: 0.17501632124185562, G Loss: 1.7736473083496094\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Epoch 22 / 100, D Loss: 0.215435691177845, G Loss: 1.8446604013442993\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Epoch 23 / 100, D Loss: 0.2094559669494629, G Loss: 1.8667093515396118\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Epoch 24 / 100, D Loss: 0.16375048458576202, G Loss: 1.9547194242477417\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch 25 / 100, D Loss: 0.2214447557926178, G Loss: 2.1063284873962402\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch 26 / 100, D Loss: 0.16945835947990417, G Loss: 2.0906949043273926\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Epoch 27 / 100, D Loss: 0.1882101446390152, G Loss: 2.11277437210083\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch 28 / 100, D Loss: 0.16757728159427643, G Loss: 2.2536587715148926\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Epoch 29 / 100, D Loss: 0.15544462203979492, G Loss: 2.3130903244018555\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch 30 / 100, D Loss: 0.12085846439003944, G Loss: 2.2236385345458984\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch 31 / 100, D Loss: 0.12676169723272324, G Loss: 2.2998921871185303\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Epoch 32 / 100, D Loss: 0.12168151512742043, G Loss: 2.377408504486084\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch 33 / 100, D Loss: 0.11276104301214218, G Loss: 2.2973740100860596\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch 34 / 100, D Loss: 0.17408046126365662, G Loss: 2.3681886196136475\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch 35 / 100, D Loss: 0.10055374726653099, G Loss: 2.5390241146087646\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Epoch 36 / 100, D Loss: 0.12321951985359192, G Loss: 2.5070767402648926\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch 37 / 100, D Loss: 0.0887560099363327, G Loss: 2.4940147399902344\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Epoch 38 / 100, D Loss: 0.1332833096385002, G Loss: 2.5547330379486084\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch 39 / 100, D Loss: 0.09462806954979897, G Loss: 2.5619072914123535\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch 40 / 100, D Loss: 0.09722220152616501, G Loss: 2.586505889892578\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Epoch 41 / 100, D Loss: 0.08568956702947617, G Loss: 2.5391957759857178\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 42 / 100, D Loss: 0.1140967309474945, G Loss: 2.72463321685791\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 43 / 100, D Loss: 0.08641986176371574, G Loss: 2.747450351715088\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 44 / 100, D Loss: 0.09564006328582764, G Loss: 2.7077178955078125\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Epoch 45 / 100, D Loss: 0.09163923189043999, G Loss: 2.686790943145752\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 46 / 100, D Loss: 0.06997822970151901, G Loss: 2.715956926345825\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 47 / 100, D Loss: 0.0949929840862751, G Loss: 2.925267219543457\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 48 / 100, D Loss: 0.09787280857563019, G Loss: 2.7493057250976562\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 49 / 100, D Loss: 0.0638200156390667, G Loss: 3.006300210952759\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 50 / 100, D Loss: 0.07878944650292397, G Loss: 2.9083101749420166\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 51 / 100, D Loss: 0.10993152111768723, G Loss: 2.9603772163391113\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Epoch 52 / 100, D Loss: 0.11200404167175293, G Loss: 2.9982595443725586\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 53 / 100, D Loss: 0.07799242064356804, G Loss: 3.091940402984619\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 54 / 100, D Loss: 0.0593694094568491, G Loss: 2.918031930923462\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 55 / 100, D Loss: 0.09326223284006119, G Loss: 3.1118030548095703\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 56 / 100, D Loss: 0.11891990713775158, G Loss: 3.151444911956787\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 57 / 100, D Loss: 0.0732826516032219, G Loss: 3.119015693664551\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 58 / 100, D Loss: 0.05643639527261257, G Loss: 3.173197031021118\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 59 / 100, D Loss: 0.10477764531970024, G Loss: 3.1871581077575684\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 60 / 100, D Loss: 0.10481330007314682, G Loss: 3.1493048667907715\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 61 / 100, D Loss: 0.10224856063723564, G Loss: 3.0947155952453613\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 62 / 100, D Loss: 0.07251664437353611, G Loss: 3.1391444206237793\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Epoch 63 / 100, D Loss: 0.14850883930921555, G Loss: 3.2281408309936523\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 64 / 100, D Loss: 0.08411275967955589, G Loss: 3.162928342819214\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Epoch 65 / 100, D Loss: 0.0827389620244503, G Loss: 3.2711429595947266\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 66 / 100, D Loss: 0.12044472619891167, G Loss: 3.542642831802368\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Epoch 67 / 100, D Loss: 0.11748898774385452, G Loss: 3.417356014251709\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 68 / 100, D Loss: 0.08067549392580986, G Loss: 3.4520702362060547\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 69 / 100, D Loss: 0.11583459377288818, G Loss: 3.451563835144043\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 70 / 100, D Loss: 0.16072367131710052, G Loss: 3.497314929962158\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Epoch 71 / 100, D Loss: 0.12780150771141052, G Loss: 3.5088181495666504\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 72 / 100, D Loss: 0.0738402009010315, G Loss: 3.457092761993408\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 73 / 100, D Loss: 0.09774889051914215, G Loss: 3.4390339851379395\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 74 / 100, D Loss: 0.11163327097892761, G Loss: 3.74186110496521\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 75 / 100, D Loss: 0.05888496898114681, G Loss: 3.606995105743408\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 76 / 100, D Loss: 0.1303158551454544, G Loss: 3.6369879245758057\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch 77 / 100, D Loss: 0.10716907680034637, G Loss: 3.574728488922119\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 78 / 100, D Loss: 0.13174786418676376, G Loss: 3.660560131072998\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch 79 / 100, D Loss: 0.13306335732340813, G Loss: 3.688483715057373\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 80 / 100, D Loss: 0.10220852121710777, G Loss: 3.857987880706787\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 81 / 100, D Loss: 0.08594262972474098, G Loss: 3.770875930786133\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Epoch 82 / 100, D Loss: 0.09309498965740204, G Loss: 3.7302684783935547\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 83 / 100, D Loss: 0.2396007664501667, G Loss: 3.7753570079803467\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 84 / 100, D Loss: 0.08751697838306427, G Loss: 3.7162065505981445\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 85 / 100, D Loss: 0.10602635890245438, G Loss: 3.850235939025879\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 86 / 100, D Loss: 0.10785769671201706, G Loss: 4.020342826843262\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 87 / 100, D Loss: 0.08981682360172272, G Loss: 3.8758091926574707\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 88 / 100, D Loss: 0.12926524132490158, G Loss: 3.9806454181671143\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 89 / 100, D Loss: 0.06856255047023296, G Loss: 3.7161922454833984\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 90 / 100, D Loss: 0.07289581000804901, G Loss: 3.735805034637451\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 91 / 100, D Loss: 0.10148077830672264, G Loss: 3.7652430534362793\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 92 / 100, D Loss: 0.11518670618534088, G Loss: 3.7250516414642334\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 93 / 100, D Loss: 0.06679045408964157, G Loss: 4.03202486038208\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Epoch 94 / 100, D Loss: 0.05699082650244236, G Loss: 3.7649312019348145\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 95 / 100, D Loss: 0.08358411118388176, G Loss: 3.758816719055176\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Epoch 96 / 100, D Loss: 0.06168335862457752, G Loss: 3.8243534564971924\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Epoch 97 / 100, D Loss: 0.053042709827423096, G Loss: 3.7487897872924805\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 98 / 100, D Loss: 0.18792829662561417, G Loss: 4.147018909454346\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 99 / 100, D Loss: 0.191519632935524, G Loss: 4.000157833099365\n"
          ]
        }
      ]
    }
  ]
}